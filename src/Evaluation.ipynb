{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa74f3b-0e7d-4e24-bdea-911688f8aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatabenchDB import DatabenchDB, DataMode\n",
    "from MultiAgentsText2SQL import State, QueryOutput, MultiAgentsText2SQL, MultiAgentTypeMode\n",
    "from MultiAgentsText2SQL_Tester import MultiAgentsText2SQL_Tester\n",
    "from Util import SaveList2File, CreateDirectory, CreateAnalysisReport\n",
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "import httpx\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from databench_eval import Runner, Evaluator\n",
    "from databench_eval.utils import load_qa\n",
    "from databench_eval.utils import load_table\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f561303-d17c-449d-8fd4-a7da82e7a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega as configurações da aplicação\n",
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('config-v1.x.ini', 'UTF-8')\n",
    "\n",
    "http_client = httpx.Client(verify='petrobras-ca-root.pem')\n",
    "\n",
    "#Instancia o LLM, que será usado pelo workflow LangGraph\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_base=config['OPENAI']['AZURE_OPENAI_BASE_URL'],\n",
    "    model_name=config['OPENAI']['CHATGPT_MODEL'],\n",
    "    openai_api_version=config['OPENAI']['OPENAI_API_VERSION'],\n",
    "    openai_api_key=config['OPENAI']['OPENAI_API_KEY'],\n",
    "    http_client=http_client,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7481634-a570-430d-a405-9f6cc7c442d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define os paths \n",
    "pathDatasets= \"/projetos/hmro/Mestrado/Semeval2025_Task8/datasets/\"\n",
    "pathDatabases= \"/projetos/hmro/Mestrado/Semeval2025_Task8/databases/\"\n",
    "pathResults = \"/projetos/hmro/Mestrado/Semeval2025_Task8/results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4126816-79cb-49cc-bcf9-c1af4d90a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega a base Question Answer de testes\n",
    "qa_test = load_qa(name=\"semeval\", split=\"test\")\n",
    "\n",
    "df_qa_test = qa_test.to_pandas()\n",
    "\n",
    "datasets = df_qa_test['dataset'].unique()\n",
    "\n",
    "datasets_ordenados = sorted(datasets)\n",
    "\n",
    "print(datasets_ordenados)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1230a0-5b14-447a-b77b-c7ccc8aae6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_ordenados = ['076_NBA', '077_Gestational', '078_Fires', '079_Coffee', '080_Books']\n",
    "\n",
    "# Define o tipo de estrutura de agentes que será executada (MultiAgentTypeMode.PLAN_AND_EXECUTE ou SIMPLE)\n",
    "multiAgentType = MultiAgentTypeMode.PLAN_AND_EXECUTE\n",
    "\n",
    "#Executa os testes com SAMPLE e ALL para todas as questões para cada dataset\n",
    "executions_mode = [DataMode.SAMPLE,DataMode.ALL]\n",
    "\n",
    "#datasets_ordenados =['066_IBM_HR','067_TripAdvisor']\n",
    "\n",
    "#Relatório consolidade com as metricas das inferencias por dataset e mode (ALL ou SAMPLE)\n",
    "df_consolidated = pd.DataFrame(columns=['dataset', 'datamode', 'numQuestions', 'acc'])\n",
    "\n",
    "execution_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "#Criar subdiretorio para os resultados deste teste\n",
    "testDirPath =f\"{pathResults}{execution_timestamp}\"\n",
    "CreateDirectory(testDirPath)\n",
    "\n",
    "for execution_mode in executions_mode:\n",
    "    \n",
    "    #Executa os testes para cada DataSet\n",
    "    for dataset_name in datasets_ordenados:\n",
    "\n",
    "        #Carrega o database\n",
    "        databenchDB = DatabenchDB(pathDatasets, pathDatabases, dataset_name, execution_mode)\n",
    "\n",
    "        #Carrega o grafo de agentes\n",
    "        multiAgentsText2SQL= MultiAgentsText2SQL(llm, multiAgentType, databenchDB)\n",
    "\n",
    "        #Carrega o testador\n",
    "        multiAgentsText2SQL_Tester = MultiAgentsText2SQL_Tester(multiAgentsText2SQL,execution_mode)\n",
    "    \n",
    "        #filtra a base teste Question and Answer pelo dataset\n",
    "        qa_test_filter = qa_test.filter(lambda x: x['dataset'] == dataset_name)\n",
    "\n",
    "\n",
    "        #Salva o dataframe com QA desta base\n",
    "        df_qa_test_filter = qa_test_filter.to_pandas()\n",
    "        df_qa_test_filter.to_csv(f\"{testDirPath}/qa_{dataset_name}_{execution_mode.name}_{execution_timestamp}.csv\")\n",
    "                \n",
    "        #Executa a inferência para todas as questões da base corrente      \n",
    "        responses = Runner(multiAgentsText2SQL_Tester.call, \n",
    "                                  qa=qa_test_filter, \n",
    "                                  prompt_generator=multiAgentsText2SQL_Tester.prompt_generator                                 \n",
    "                                 ).run( save=f\"{testDirPath}/model_responses_{dataset_name}_{execution_mode.name}_{execution_timestamp}.txt\")\n",
    "\n",
    "        #Avalia a acurácia\n",
    "        evaluator = Evaluator(qa=qa_test_filter)\n",
    "\n",
    "        #Retorna a acurácia\n",
    "        acc = evaluator.eval(responses, lite=True if (execution_mode == DataMode.SAMPLE) else False)\n",
    "\n",
    "        #Salva arquivo com o resultado das avaliações\n",
    "        SaveList2File(evaluator.evals,f\"{testDirPath}/eval_{dataset_name}_{execution_mode.name}_{execution_timestamp}.txt\")\n",
    "              \n",
    "        # Criar um DataFrame temporário com a linha que deseja adicionar\n",
    "        nova_linha = pd.DataFrame([{'dataset': dataset_name, 'datamode': execution_mode.name,'numQuestions': qa_test_filter.num_rows,'acc': acc}])\n",
    "        \n",
    "        # Concatenar os DataFrames\n",
    "        df_consolidated = pd.concat([df_consolidated, nova_linha], ignore_index=True)      \n",
    "                       \n",
    "        \n",
    "# Salvar como arquivo CSV o relatorio consolidado com a acuracia de todas as bases de teste\n",
    "df_consolidated.to_csv(f\"{testDirPath}/relatorio_avaliacao_{execution_timestamp}.csv\", index=False)       \n",
    "\n",
    "# Salvar como arquivo CSV o consolidado de todas as questões, respostas de referencia, comparadas com as \n",
    "# respostas do modelo e avaliação para as bases SAMPLE e FULL\n",
    "CreateAnalysisReport(testDirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a1b3a-8282-4c76-97cd-330a8d0ec981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega o database\n",
    "dataset_name = '066_IBM_HR'\n",
    "execution_mode= DataMode.SAMPLE\n",
    "databenchDB = DatabenchDB(pathDatasets, pathDatabases, dataset_name, execution_mode)\n",
    "\n",
    "#Carrega o grafo de agentes\n",
    "multiAgentsText2SQL= MultiAgentsText2SQL(llm, MultiAgentTypeMode.PLAN_AND_EXECUTE, databenchDB)\n",
    "\n",
    "multiAgentsText2SQL.Invoke(\"Are there more employees who travel frequently than those who work in the HR department?\")           \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mestrado-Semeval2025-Task8",
   "language": "python",
   "name": "semeval2025_task8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
